---
title: "CEG5304 Week 3 Coding Cheatsheet"
date: 2025-11-07T10:30:00+08:00
weight: 400
tags: ["CEG5304", "Week3", "Deep Learning", "MLP", "Perceptron", "Backpropagation", "Cheatsheet", "Coding", "Exam", "NumPy"]
categories: ["CEG5304"]
draft: false
summary: "Deep learning fundamentals including perceptron, MLP, activation functions, backpropagation, regularization, and training techniques with NumPy."
---

> Deep Learning fundamentals: Perceptron, MLP, Backpropagation, Regularization  
> All code is **pure NumPy**, no torch.

---

## 1. åŸºç¡€å¯¼å…¥å’Œè®¾ç½®

```python
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®éšæœºç§å­ï¼ˆå¯å¤ç°ï¼‰
np.random.seed(42)

# å¸¸ç”¨å‚æ•°
learning_rate = 0.01
epochs = 1000
batch_size = 32
```

---

## 2. æ¿€æ´»å‡½æ•° & å¯¼æ•°

### Sigmoid

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)
```

### Tanh

```python
def tanh(z):
    return np.tanh(z)
    # æˆ–: (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))

def tanh_derivative(z):
    return 1 - np.tanh(z)**2
```

### ReLU

```python
def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z > 0).astype(float)
```

---

## 3. æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰

### å•ä¸ªæ„ŸçŸ¥æœº

```python
class Perceptron:
    def __init__(self, input_size):
        # åˆå§‹åŒ–æƒé‡ (åŒ…æ‹¬bias)
        self.weights = np.random.randn(input_size + 1) * 0.01
    
    def predict(self, X):
        # æ·»åŠ biasé¡¹ (x0 = 1)
        X_bias = np.c_[np.ones(X.shape[0]), X]
        # è®¡ç®—åŠ æƒå’Œ
        z = np.dot(X_bias, self.weights)
        # é˜¶è·ƒå‡½æ•°
        return (z > 0).astype(int)
    
    def fit(self, X, y, epochs=100, lr=0.01):
        X_bias = np.c_[np.ones(X.shape[0]), X]
        
        for epoch in range(epochs):
            for i in range(len(X)):
                # é¢„æµ‹
                z = np.dot(X_bias[i], self.weights)
                y_pred = 1 if z > 0 else 0
                
                # æ›´æ–°æƒé‡: w = w + (d - y) * x
                error = y[i] - y_pred
                self.weights += lr * error * X_bias[i]
        
        return self
```

### æ„ŸçŸ¥æœºä½¿ç”¨ç¤ºä¾‹

```python
# AND gate
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([0, 0, 0, 1])

perceptron = Perceptron(input_size=2)
perceptron.fit(X, y, epochs=100)
predictions = perceptron.predict(X)
```

---

## 4. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

### å®Œæ•´MLPå®ç°

```python
class MLP:
    def __init__(self, layer_sizes):
        """
        layer_sizes: list, e.g., [2, 4, 3, 1] 
                     (input, hidden1, hidden2, output)
        """
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.weights = []
        self.biases = []
        
        for i in range(len(layer_sizes) - 1):
            # He initialization
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0/layer_sizes[i])
            b = np.zeros((1, layer_sizes[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.activations = [X]  # å­˜å‚¨æ¯å±‚çš„æ¿€æ´»å€¼
        self.z_values = []       # å­˜å‚¨æ¯å±‚çš„åŠ æƒå’Œ
        
        A = X
        for i in range(len(self.weights)):
            Z = np.dot(A, self.weights[i]) + self.biases[i]
            self.z_values.append(Z)
            
            # æœ€åä¸€å±‚ç”¨sigmoidï¼Œå…¶ä»–å±‚ç”¨ReLU
            if i == len(self.weights) - 1:
                A = sigmoid(Z)
            else:
                A = relu(Z)
            
            self.activations.append(A)
        
        return A
    
    def backward(self, X, y):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]  # batch size
        
        # åˆå§‹åŒ–æ¢¯åº¦
        dW = [np.zeros_like(w) for w in self.weights]
        db = [np.zeros_like(b) for b in self.biases]
        
        # è¾“å‡ºå±‚è¯¯å·®
        dA = self.activations[-1] - y  # å¯¹äºMSE loss
        
        # ä»åå¾€å‰ä¼ æ’­
        for i in reversed(range(len(self.weights))):
            # å½“å‰å±‚çš„åŠ æƒå’Œ
            Z = self.z_values[i]
            
            # æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
            if i == len(self.weights) - 1:
                dZ = dA * sigmoid_derivative(Z)
            else:
                dZ = dA * relu_derivative(Z)
            
            # è®¡ç®—æ¢¯åº¦
            dW[i] = np.dot(self.activations[i].T, dZ) / m
            db[i] = np.sum(dZ, axis=0, keepdims=True) / m
            
            # ä¼ æ’­åˆ°å‰ä¸€å±‚
            if i > 0:
                dA = np.dot(dZ, self.weights[i].T)
        
        return dW, db
    
    def update_weights(self, dW, db, lr):
        """æ›´æ–°æƒé‡"""
        for i in range(len(self.weights)):
            self.weights[i] -= lr * dW[i]
            self.biases[i] -= lr * db[i]
    
    def train(self, X, y, epochs=1000, lr=0.01, batch_size=32):
        """è®­ç»ƒ"""
        losses = []
        n_samples = X.shape[0]
        
        for epoch in range(epochs):
            # Mini-batchæ¢¯åº¦ä¸‹é™
            indices = np.random.permutation(n_samples)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            epoch_loss = 0
            for i in range(0, n_samples, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                # å‰å‘ä¼ æ’­
                y_pred = self.forward(X_batch)
                
                # è®¡ç®—æŸå¤±
                loss = np.mean((y_pred - y_batch)**2)
                epoch_loss += loss
                
                # åå‘ä¼ æ’­
                dW, db = self.backward(X_batch, y_batch)
                
                # æ›´æ–°æƒé‡
                self.update_weights(dW, db, lr)
            
            losses.append(epoch_loss)
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")
        
        return losses
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.forward(X)
```

### MLPä½¿ç”¨ç¤ºä¾‹

```python
# XORé—®é¢˜
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])

# åˆ›å»ºç½‘ç»œ: 2è¾“å…¥ -> 4éšè— -> 1è¾“å‡º
mlp = MLP([2, 4, 1])
losses = mlp.train(X, y, epochs=5000, lr=0.1)

# é¢„æµ‹
predictions = mlp.predict(X)
print("Predictions:", predictions.round())
```

---

## 5. æŸå¤±å‡½æ•°

### Mean Squared Error (MSE)

```python
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

def mse_loss_derivative(y_true, y_pred):
    return 2 * (y_pred - y_true) / y_true.shape[0]
```

### Binary Cross-Entropy

```python
def binary_crossentropy(y_true, y_pred):
    epsilon = 1e-15  # é˜²æ­¢log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def binary_crossentropy_derivative(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -(y_true / y_pred - (1 - y_true) / (1 - y_pred)) / y_true.shape[0]
```

---

## 6. æ¢¯åº¦ä¸‹é™å˜ä½“

### Batch Gradient Descent

```python
def batch_gd(X, y, weights, lr, epochs):
    for epoch in range(epochs):
        # ä½¿ç”¨æ‰€æœ‰æ•°æ®
        y_pred = forward(X, weights)
        loss = compute_loss(y, y_pred)
        
        # è®¡ç®—æ¢¯åº¦
        grads = compute_gradients(X, y, y_pred)
        
        # æ›´æ–°æƒé‡
        weights -= lr * grads
    
    return weights
```

### Stochastic Gradient Descent

```python
def sgd(X, y, weights, lr, epochs):
    n = X.shape[0]
    
    for epoch in range(epochs):
        for i in range(n):
            # å•ä¸ªæ ·æœ¬
            Xi = X[i:i+1]
            yi = y[i:i+1]
            
            y_pred = forward(Xi, weights)
            grads = compute_gradients(Xi, yi, y_pred)
            
            # æ›´æ–°æƒé‡
            weights -= lr * grads
    
    return weights
```

### Mini-batch Gradient Descent

```python
def mini_batch_gd(X, y, weights, lr, epochs, batch_size=32):
    n = X.shape[0]
    
    for epoch in range(epochs):
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(n)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, n, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            y_pred = forward(X_batch, weights)
            grads = compute_gradients(X_batch, y_batch, y_pred)
            
            # æ›´æ–°æƒé‡
            weights -= lr * grads
    
    return weights
```

---

## 7. æ­£åˆ™åŒ–æŠ€æœ¯

### Dropout

```python
def dropout(X, keep_prob=0.5, training=True):
    """
    X: è¾“å…¥
    keep_prob: ä¿ç•™æ¦‚ç‡ (0.5 è¡¨ç¤ºdropout 50%)
    training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
    """
    if not training:
        return X
    
    # ç”Ÿæˆmask
    mask = np.random.rand(*X.shape) < keep_prob
    
    # åº”ç”¨maskå¹¶ç¼©æ”¾
    return (X * mask) / keep_prob  # inverted dropout

# ä½¿ç”¨ç¤ºä¾‹
class MLPWithDropout(MLP):
    def forward(self, X, training=True, keep_prob=0.5):
        self.activations = [X]
        self.z_values = []
        
        A = X
        for i in range(len(self.weights)):
            Z = np.dot(A, self.weights[i]) + self.biases[i]
            self.z_values.append(Z)
            
            if i == len(self.weights) - 1:
                A = sigmoid(Z)
            else:
                A = relu(Z)
                # åœ¨éšè—å±‚åº”ç”¨dropout
                if training:
                    A = dropout(A, keep_prob, training=True)
            
            self.activations.append(A)
        
        return A
```

### Weight Decay (L2 Regularization)

```python
def compute_loss_with_l2(y_true, y_pred, weights, lambda_reg=0.01):
    """
    lambda_reg: æ­£åˆ™åŒ–ç³»æ•°
    """
    # åŸºç¡€æŸå¤±
    loss = mse_loss(y_true, y_pred)
    
    # L2æ­£åˆ™åŒ–é¡¹
    l2_penalty = 0
    for w in weights:
        l2_penalty += np.sum(w**2)
    
    return loss + (lambda_reg / 2) * l2_penalty

# æ¢¯åº¦æ›´æ–°æ—¶åŠ å…¥æƒé‡è¡°å‡
def update_weights_with_decay(weights, grads, lr, lambda_reg=0.01):
    for i in range(len(weights)):
        # w = w - lr * (grad + lambda * w)
        weights[i] -= lr * (grads[i] + lambda_reg * weights[i])
    
    return weights
```

### Early Stopping

```python
def train_with_early_stopping(model, X_train, y_train, X_val, y_val, 
                               epochs=1000, patience=10):
    best_val_loss = float('inf')
    patience_counter = 0
    best_weights = None
    
    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train_one_epoch(X_train, y_train)
        
        # éªŒè¯
        val_loss = model.evaluate(X_val, y_val)
        
        # æ£€æŸ¥æ˜¯å¦æ”¹å–„
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_weights = model.get_weights()  # ä¿å­˜æœ€ä½³æƒé‡
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            model.set_weights(best_weights)  # æ¢å¤æœ€ä½³æƒé‡
            break
    
    return model
```

---

## 8. æ•°æ®é›†åˆ’åˆ†

### Train/Validation/Test Split

```python
def train_val_test_split(X, y, train_ratio=0.7, val_ratio=0.15):
    """
    train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
    val_ratio: éªŒè¯é›†æ¯”ä¾‹
    test_ratio: 1 - train_ratio - val_ratio
    """
    n = len(X)
    indices = np.random.permutation(n)
    
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))
    
    train_idx = indices[:train_end]
    val_idx = indices[train_end:val_end]
    test_idx = indices[val_end:]
    
    return (X[train_idx], y[train_idx],
            X[val_idx], y[val_idx],
            X[test_idx], y[test_idx])

# ä½¿ç”¨ç¤ºä¾‹
X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y)
```

### K-Fold Cross-Validation

```python
def k_fold_split(X, y, k=5):
    """
    è¿”å›kä¸ª(train, val)ç´¢å¼•å¯¹
    """
    n = len(X)
    indices = np.random.permutation(n)
    fold_size = n // k
    
    folds = []
    for i in range(k):
        val_start = i * fold_size
        val_end = (i + 1) * fold_size if i < k - 1 else n
        
        val_idx = indices[val_start:val_end]
        train_idx = np.concatenate([indices[:val_start], indices[val_end:]])
        
        folds.append((train_idx, val_idx))
    
    return folds

# ä½¿ç”¨ç¤ºä¾‹
def cross_validate(X, y, k=5):
    folds = k_fold_split(X, y, k)
    scores = []
    
    for train_idx, val_idx in folds:
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        
        model = MLP([X.shape[1], 4, 1])
        model.train(X_train, y_train, epochs=100)
        
        val_loss = mse_loss(y_val, model.predict(X_val))
        scores.append(val_loss)
    
    return np.mean(scores), np.std(scores)
```

---

## 9. å¸¸ç”¨å·¥å…·å‡½æ•°

### æ•°æ®æ ‡å‡†åŒ–

```python
def standardize(X):
    """Z-scoreæ ‡å‡†åŒ–"""
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X - mean) / (std + 1e-8), mean, std

def normalize(X):
    """Min-Maxå½’ä¸€åŒ–åˆ°[0,1]"""
    min_val = np.min(X, axis=0)
    max_val = np.max(X, axis=0)
    return (X - min_val) / (max_val - min_val + 1e-8)
```

### One-Hotç¼–ç 

```python
def one_hot_encode(y, num_classes):
    """
    y: shape (n,)
    return: shape (n, num_classes)
    """
    n = len(y)
    one_hot = np.zeros((n, num_classes))
    one_hot[np.arange(n), y] = 1
    return one_hot
```

### å‡†ç¡®ç‡è®¡ç®—

```python
def accuracy(y_true, y_pred):
    """
    åˆ†ç±»å‡†ç¡®ç‡
    """
    return np.mean((y_pred > 0.5) == y_true)

def confusion_matrix(y_true, y_pred):
    """
    æ··æ·†çŸ©é˜µ
    """
    y_pred_binary = (y_pred > 0.5).astype(int)
    
    TP = np.sum((y_true == 1) & (y_pred_binary == 1))
    TN = np.sum((y_true == 0) & (y_pred_binary == 0))
    FP = np.sum((y_true == 0) & (y_pred_binary == 1))
    FN = np.sum((y_true == 1) & (y_pred_binary == 0))
    
    return np.array([[TN, FP], [FN, TP]])
```

---

## 10. å¿«é€Ÿæµ‹è¯•æ¨¡æ¿

```python
# ===== å¿«é€Ÿæµ‹è¯•æ¨¡æ¿ =====

# 1. å‡†å¤‡æ•°æ®
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])  # XOR

# 2. æ ‡å‡†åŒ– (å¯é€‰)
X_norm, mean, std = standardize(X)

# 3. åˆ’åˆ†æ•°æ®é›†
X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y)

# 4. åˆ›å»ºæ¨¡å‹
model = MLP([2, 4, 1])  # 2è¾“å…¥ -> 4éšè— -> 1è¾“å‡º

# 5. è®­ç»ƒ
losses = model.train(X_train, y_train, epochs=1000, lr=0.1, batch_size=2)

# 6. è¯„ä¼°
train_pred = model.predict(X_train)
val_pred = model.predict(X_val)

print("Training Accuracy:", accuracy(y_train, train_pred))
print("Validation Accuracy:", accuracy(y_val, val_pred))

# 7. å¯è§†åŒ–
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
```

---

## 11. è€ƒè¯•æ³¨æ„äº‹é¡¹

### âœ… å¿…é¡»ä¼šçš„

- Sigmoid/ReLU/TanhåŠå…¶å¯¼æ•°
- æ„ŸçŸ¥æœºæƒé‡æ›´æ–°å…¬å¼
- å‰å‘ä¼ æ’­ï¼ˆæ‰‹å†™çŸ©é˜µä¹˜æ³•ï¼‰
- åå‘ä¼ æ’­ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
- Mini-batché‡‡æ ·
- MSEæŸå¤±å‡½æ•°

### âš ï¸ å¸¸è§é”™è¯¯

```python
# âŒ å¿˜è®°æ·»åŠ bias
Z = np.dot(X, W)  # é”™è¯¯

# âœ… æ­£ç¡®
Z = np.dot(X, W) + b

# âŒ ç»´åº¦ä¸åŒ¹é…
W = np.random.randn(4, 2)  # é”™è¯¯ (åº”è¯¥æ˜¯2x4)

# âœ… æ£€æŸ¥ç»´åº¦
print(f"X: {X.shape}, W: {W.shape}, Z: {Z.shape}")

# âŒ Dropoutæ—¶å¿˜è®°ç¼©æ”¾
A = A * mask  # é”™è¯¯

# âœ… Inverted dropout
A = (A * mask) / keep_prob
```

### ğŸ” è°ƒè¯•æŠ€å·§

```python
# æ‰“å°ä¸­é—´å€¼
def forward_debug(X, weights):
    print(f"Input shape: {X.shape}")
    Z = np.dot(X, weights)
    print(f"Z shape: {Z.shape}")
    print(f"Z sample: {Z[:3]}")  # æŸ¥çœ‹å‰3ä¸ªå€¼
    return Z

# æ£€æŸ¥æ¢¯åº¦ï¼ˆæ•°å€¼æ¢¯åº¦ï¼‰
def numerical_gradient(f, x, h=1e-5):
    grad = np.zeros_like(x)
    for i in range(len(x)):
        old_value = x[i]
        
        x[i] = old_value + h
        fxh1 = f(x)
        
        x[i] = old_value - h
        fxh2 = f(x)
        
        grad[i] = (fxh1 - fxh2) / (2 * h)
        x[i] = old_value
    
    return grad
```

---
